{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M5 Forecasting Competition GluonTS TemplateÂ¶\n",
    "This notebook can be used as a starting point for participating in the M5 forecasting competition using GluonTS-based tooling.\n",
    "\n",
    "\n",
    "M5 Forecasting - Accuracy source image\n",
    "M5 Forecasting - Accuracy\n",
    "Estimate the unit sales of Walmart retail goods\n",
    "Last Updated: 2 months ago\n",
    "About this Competition\n",
    "In the challenge, you are predicting item sales at stores in various locations for two 28-day time periods. Information about the data is found in the M5 Participants Guide.\n",
    "\n",
    "Files\n",
    "calendar.csv - Contains information about the dates on which the products are sold.\n",
    "sales_train_validation.csv - Contains the historical daily unit sales data per product and store [d_1 - d_1913]\n",
    "sample_submission.csv - The correct format for submissions. Reference the Evaluation tab for more info.\n",
    "sell_prices.csv - Contains information about the price of the products sold per store and date.\n",
    "sales_train_evaluation.csv - Available once month before competition deadline. Will include sales [d_1 - d_1941]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mxnet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-59c17f90cc68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m##########################################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# %matplotlib inline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmxnet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmxnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgluon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mxnet'"
     ]
    }
   ],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from tqdm.autonotebook import tqdm\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define globally accessible variables, such as the pred length and the input path for the M5 data.\n",
    " Note that single_pred_length corresponds to the length of the val/evaluation periods, while submission_pred_length corresponds to the length of both these periods combined.\n",
    "\n",
    "By default the notebook is configured to run in submission mode (submission will be True), \n",
    "which means that we use all of the data for training and predict new values for a \n",
    "total length of submission_pred_length for which we don't have ground truth values available\n",
    " (performance can be assessed by submitting pred results to Kaggle). \n",
    " In contrast, setting submission to False will instead use the last single_pred_length-many\n",
    "  values of our training set as val points (and hence these values will not be used for training),\n",
    "   which enables us to validate our model's performance offline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_pred_length = 28\n",
    "submission_pred_length = single_pred_length * 2\n",
    "m5_input_path=\"./m5-forecasting-accuracy\"\n",
    "submission=True\n",
    "\n",
    "if submission:\n",
    "    pred_length = submission_pred_length\n",
    "else:\n",
    "    pred_length = single_pred_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the M5 data into GluonTS\n",
    "First we need to convert the provided M5 data into a format that is readable by GluonTS.\n",
    " At this point we assume that the M5 data, which can be downloaded from Kaggle, is present under m5_input_path.\n",
    "\n",
    "MultiVariat Dataset\n",
    "\n",
    "Files\n",
    "calendar.csv               : Contains information about the dates on which the products are sold.\n",
    "sales_train_validation.csv : Contains the historical daily unit sales data per product and store [d_1 - d_1913]\n",
    "sample_submission.csv      : The correct format for submissions. Reference the Evaluation tab for more info.\n",
    "sell_prices.csv            : Contains information about the price of the products sold per store and date.\n",
    "sales_train_evaluation.csv : Available once month before competition deadline. Will include sales [d_1 - d_1941]\n",
    "\n",
    "\n",
    "https://www.kaggle.com/steverab/m5-forecasting-competition-gluonts-template\n",
    "\n",
    "\n",
    "(ID x timeStamp ) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-756c8e3f4893>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcalendar\u001b[0m               \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{m5_input_path}/calendar.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msales_train_val\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{m5_input_path}/sales_train_val.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msample_submission\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{m5_input_path}/sample_submission.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msell_prices\u001b[0m            \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{m5_input_path}/sell_prices.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "calendar               = pd.read_csv(f'{m5_input_path}/calendar.csv')\n",
    "sales_train_val        = pd.read_csv(f'{m5_input_path}/sales_train_val.csv')\n",
    "sample_submission      = pd.read_csv(f'{m5_input_path}/sample_submission.csv')\n",
    "sell_prices            = pd.read_csv(f'{m5_input_path}/sell_prices.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start the data convertion process by building dynamic features \n",
    "(features that change over time, just like the target values). \n",
    "Here, we are mainly interested in the event indicators event_type_1 and event_type_2. \n",
    "We will mostly drop dynamic time features as GluonTS will automatically add \n",
    "some of these as part of many models' transformation chains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dynamic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_feat = calendar.drop(\n",
    "    ['date', 'wm_yr_wk', 'weekday', 'wday', 'month', 'year', 'event_name_1', 'event_name_2', 'd'], \n",
    "    axis=1\n",
    ")\n",
    "cal_feat['event_type_1'] = cal_feat['event_type_1'].apply(lambda x: 0 if str(x)==\"nan\" else 1)\n",
    "cal_feat['event_type_2'] = cal_feat['event_type_2'].apply(lambda x: 0 if str(x)==\"nan\" else 1)\n",
    "\n",
    "test_cal_feat = cal_feat.values.T\n",
    "if submission:\n",
    "    train_cal_feat = test_cal_feat[:,:-submission_pred_length]\n",
    "else:\n",
    "    train_cal_feat = test_cal_feat[:,:-submission_pred_length-single_pred_length]\n",
    "    test_cal_feat  = test_cal_feat[:,:-submission_pred_length]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of individual time series   Nb Series x Lenght_time_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cal_feat_list  = [test_cal_feat] * len(sales_train_val)\n",
    "train_cal_feat_list = [train_cal_feat] * len(sales_train_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Static Features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_ids                       = sales_train_val[\"state_id\"].astype('category').cat.codes.values\n",
    "state_ids_un , state_ids_counts = np.unique(state_ids, return_counts=True)\n",
    "\n",
    "store_ids                       = sales_train_val[\"store_id\"].astype('category').cat.codes.values\n",
    "store_ids_un , store_ids_counts = np.unique(store_ids, return_counts=True)\n",
    "\n",
    "cat_ids                         = sales_train_val[\"cat_id\"].astype('category').cat.codes.values\n",
    "cat_ids_un , cat_ids_counts     = np.unique(cat_ids, return_counts=True)\n",
    "\n",
    "dept_ids                        = sales_train_val[\"dept_id\"].astype('category').cat.codes.values\n",
    "dept_ids_un , dept_ids_counts   = np.unique(dept_ids, return_counts=True)\n",
    "\n",
    "item_ids                        = sales_train_val[\"item_id\"].astype('category').cat.codes.values\n",
    "item_ids_un , item_ids_counts   = np.unique(item_ids, return_counts=True)\n",
    "\n",
    "\n",
    "\n",
    "##### Static Features \n",
    "static_cat_list          = [item_ids, dept_ids, cat_ids, store_ids, state_ids]\n",
    "static_cat               = np.concatenate(static_cat_list)\n",
    "static_cat               = static_cat.reshape(len(static_cat_list), len(item_ids)).T\n",
    "static_cat_cardinalities = [len(item_ids_un), len(dept_ids_un), len(cat_ids_un), len(store_ids_un), len(state_ids_un)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, we can build both the training and the testing set from target values and both static and dynamic features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######  Time series ##################\n",
    "from gluonts.dataset.common import load_datasets, ListDataset\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "\n",
    "\n",
    "#### Remove Categories colum\n",
    "train_df            = sales_train_val.drop([\"id\",\"item_id\",\"dept_id\",\"cat_id\",\"store_id\",\"state_id\"], axis=1)\n",
    "train_target_values = train_df.values\n",
    "\n",
    "if submission == True:\n",
    "    test_target_values = [np.append(ts, np.ones(submission_pred_length) * np.nan) for ts in train_df.values]\n",
    "\n",
    "else:\n",
    "\n",
    "    #### List of individual timeseries\n",
    "    test_target_values  = train_target_values.copy()\n",
    "    train_target_values = [ts[:-single_pred_length] for ts in train_df.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Start Dates for each time series\n",
    "m5_dates = [pd.Timestamp(\"2011-01-29\", freq='1D') for _ in range(len(sales_train_val))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ListDataset([\n",
    "    {\n",
    "        FieldName.TARGET            : target,\n",
    "        FieldName.START             : start,\n",
    "        FieldName.FEAT_DYNAMIC_REAL : fdr,\n",
    "        FieldName.FEAT_STATIC_CAT   : fsc\n",
    "    } for (target, start, fdr, fsc) in zip(train_target_values,   # list of individual time series\n",
    "                                           m5_dates,              # list of start dates\n",
    "                                           train_cal_feat_list,   # List of Dynamic Features\n",
    "                                           static_cat)              # List of Static Features \n",
    "    ],     freq=\"D\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_ds = ListDataset([\n",
    "    {\n",
    "        FieldName.TARGET            : target,\n",
    "        FieldName.START             : start,\n",
    "        FieldName.FEAT_DYNAMIC_REAL : fdr,\n",
    "        FieldName.FEAT_STATIC_CAT   : fsc\n",
    "    }\n",
    "    for (target, start, fdr, fsc) in zip(test_target_values,\n",
    "                                         m5_dates,\n",
    "                                         test_cal_feat_list,\n",
    "                                         static_cat)\n",
    "], freq=\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just to be sure, we quickly verify that dataset format is correct and that our dataset does indeed \n",
    "# contain the correct target values as well as dynamic and static features.\n",
    "########################\n",
    "next(iter(train_ds))\n",
    "\n",
    "\n",
    "###########################################################################################################\n",
    "###########################################################################################################\n",
    "\n",
    "\n",
    "def gluonts_create_dynamic(df_dynamic, submission=1, single_pred_length=28, submission_pred_length=10, n_timeseries=1, transpose=1) :\n",
    "    \"\"\"\n",
    "        N_cat x N-timseries\n",
    "    \"\"\"\n",
    "    v = df_dynamic.values.T if transpose else df_dynamic.values\n",
    "\n",
    "    train_cal_feat = v[:,:-submission_pred_length-single_pred_length]\n",
    "    test_cal_feat  = v[:,:-submission_pred_length]\n",
    "\n",
    "    #### List of individual time series   Nb Series x Lenght_time_series\n",
    "    test_list  = [test_cal_feat] * n_timeseries\n",
    "    train_list = [train_cal_feat] * n_timeseries\n",
    "    return train_list, test_list\n",
    "\n",
    "\n",
    "def gluonts_create_static(df_static, submission=1, single_pred_length=28, submission_pred_length=10, n_timeseries=1, transpose=1) :\n",
    "    \"\"\"\n",
    "        N_cat x N-timseries\n",
    "\n",
    "    \"\"\"\n",
    "    ####### Static Features \n",
    "    for col in df_static :\n",
    "      v_col  = df_static[col].astype('category').cat.codes.values\n",
    "      static_cat_list.append(v_col)\n",
    "\n",
    "\n",
    "    static_cat               = np.concatenate(static_cat_list)\n",
    "    static_cat               = static_cat.reshape(len(static_cat_list), n_timeseries).T\n",
    "    # static_cat_cardinalities = [len(df_feature_static[col].unique()) for col in df_feature_static]\n",
    "    return static_cat, static_cat\n",
    "\n",
    "\n",
    "def gluonts_create_timeseries(df_timeseries, submission=1, single_pred_length=28, submission_pred_length=10, n_timeseries=1, transpose=1) :\n",
    "    \"\"\"\n",
    "        N_cat x N-timseries\n",
    "    \"\"\"\n",
    "    #### Remove Categories colum\n",
    "    train_target_values = df_timeseries.values\n",
    "\n",
    "    if submission == True:\n",
    "        test_target_values = [np.append(ts, np.ones(submission_pred_length) * np.nan) for ts in df_timeseries.values]\n",
    "\n",
    "\n",
    "    else:\n",
    "        #### List of individual timeseries\n",
    "        test_target_values  = train_target_values.copy()\n",
    "        train_target_values = [ts[:-single_pred_length] for ts in df_timeseries.values]\n",
    "\n",
    "    return train_target_values, test_target_values\n",
    "\n",
    "\n",
    "\n",
    "#### Start Dates for each time series\n",
    "def create_startdate(date=\"2011-01-29\", freq=\"1D\", n_timeseries=1):\n",
    "   start_dates_list = [pd.Timestamp(date, freq=freq) for _ in range(n_timeseries)]\n",
    "   return start_dates_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gluonts_create_dataset(train_timeseries_list, start_dates_list, train_dynamic_list,  train_static_list, freq=\"D\" ) :\n",
    "    from gluonts.dataset.common import load_datasets, ListDataset\n",
    "    from gluonts.dataset.field_names import FieldName\n",
    "    train_ds = ListDataset([\n",
    "        {\n",
    "            FieldName.TARGET            : target,\n",
    "            FieldName.START             : start,\n",
    "            FieldName.FEAT_DYNAMIC_REAL : fdr,\n",
    "            FieldName.FEAT_STATIC_CAT   : fsc\n",
    "        } for (target, start, fdr, fsc) in zip(train_timeseries_list,   # list of individual time series\n",
    "                                               start_dates_list,              # list of start dates\n",
    "                                               train_dynamic_list,   # List of Dynamic Features\n",
    "                                               train_static_list)              # List of Static Features \n",
    "        ],     freq=freq)\n",
    "    return train_ds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######## Dataset generation\n",
    "n_timeseries           = len(sales_train_val)\n",
    "single_pred_length     = 28\n",
    "submission_pred_length = single_pred_length * 2\n",
    "startdate              = \"2011-01-29\"\n",
    "freq                   = \"1D\"\n",
    "submission= 0\n",
    "\n",
    "\n",
    "cal_feat = calendar.drop( ['date', 'wm_yr_wk', 'weekday', 'wday', 'month', 'year', 'event_name_1', 'event_name_2', 'd'],  axis=1 )\n",
    "cal_feat['event_type_1'] = cal_feat['event_type_1'].apply(lambda x: 0 if str(x)==\"nan\" else 1)\n",
    "cal_feat['event_type_2'] = cal_feat['event_type_2'].apply(lambda x: 0 if str(x)==\"nan\" else 1)\n",
    "\n",
    "\n",
    "df_dynamic    = cal_feat\n",
    "df_static     = sales_train_val[\"item_id\",\"dept_id\",\"cat_id\",\"store_id\",\"state_id\"]\n",
    "df_timeseries = sales_train_val.drop([\"id\",\"item_id\",\"dept_id\",\"cat_id\",\"store_id\",\"state_id\"], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "def pandas_to_gluonts_multiseries(df_timeseries, df_dynamic, df_static, pars=None) :\n",
    "\n",
    "    submission             = pars['submission']\n",
    "    single_pred_length     = pars['single_pred_length']\n",
    "    submission_pred_length = pars['submission_pred_length']\n",
    "    n_timeseries           = pars['n_timeseries']\n",
    "    start_date             = pars['start_date']\n",
    "\n",
    "    train_dynamic_list, test_dynamic_list       = gluonts_create_dynamic(df_dynamic, submission=submission, single_pred_length=single_pred_length, \n",
    "                                                                         submission_pred_length=submission_pred_length, n_timeseries=n_timeseries, transpose=1)\n",
    "\n",
    "\n",
    "    train_static_list, test_static_list          = gluonts_create_static(df_static , submission=submission, single_pred_length=single_pred_length, \n",
    "                                                                         submission_pred_length=submission_pred_length, n_timeseries=n_timeseries, transpose=0)\n",
    "\n",
    "\n",
    "    train_timeseries_list, test_timeseries_list = gluonts_create_timeseries(df_timeseries, submission=submission, single_pred_length=single_pred_length, \n",
    "                                                                            submission_pred_length=submission_pred_length, n_timeseries=n_timeseries, transpose=0)\n",
    "\n",
    "    start_dates_list = create_startdate(date=start_date, freq=freq, n_timeseries=1)\n",
    "\n",
    "    train_ds = gluonts_create_dataset(train_timeseries_list, start_dates_list, train_dynamic_list, train_static_list, freq=freq ) \n",
    "    test_ds  = gluonts_create_dataset(test_timeseries_list,  start_dates_list, test_dynamic_list,  test_static_list,  freq=freq ) \n",
    "    \n",
    "    return train_ds, test_ds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_ds, test_ds = pandas_to_gluonts_multiseries(df_timeseries, df_dynamic, df_static, pars=None) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
